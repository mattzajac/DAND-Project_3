{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling Report\n",
    "\n",
    "<span style=\"color: gray; font-size:1em;\">Mateusz Zajac</span>\n",
    "<br><span style=\"color: gray; font-size:1em;\">Feb-2019</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Part I - Gathering Data](#gather)\n",
    "- [Part II - Assessing Data](#assess)\n",
    "- [Part III - Cleaning Data](#clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gather'></a>\n",
    "### Part I - Gathering Data\n",
    "For this project I used data from three sources:\n",
    " 1. The WeRateDogs Twitter archive, which download manually from: [twitter_archive_enhanced.csv](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv)\n",
    " 2. The tweet image predictions, which I downloaded programmatically from:  [image_predictions.tsv](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv) using the Requests library\n",
    " 3. The tweet_json.txt, which is a downloaded JSON data (using Python's Tweepy library) from Twitter API for each tweet from the Twitter archive file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assess'></a>\n",
    "### Part II - Assessing  Data\n",
    "\n",
    "At this stage I assessed data visually and programmatically, using pandas .sample(10), .info(), .describe(), .value_counts() or .duplicated().sum(). I have identified and documented several issues with:\n",
    "\n",
    "\n",
    "<span style=\"color:red; font-size:1em;\">**Data Quality:**</span>\n",
    "\n",
    "**Twitter Archive**\n",
    " 1. dataset contains retweets\n",
    " 2. in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id should be intergs instead of float (check if fields are essential for the analysis)\n",
    " 3. timestamp and retweeted_status_timestamp should be datetime instead of object (string)\n",
    " 4. incorrect dog names (for instance: 'such', 'a', 'an')\n",
    " 5. the rating numerator and denominator have ratings above the standard scale (for instance numerator equal to 1776 and denominator equal to 170)\n",
    " 6. urls appear in the source column\n",
    " 7. source and dog stage datatype can be set to 'category'\n",
    "\n",
    "**Twitter Images**\n",
    " 1. Missing records: 2075 rows instead of 2356\n",
    " 2. 66 tweet_ids have the same duplicated jpg_urls\n",
    " 3. Natural network didn't recognize a dog in any of the attempts\n",
    "\n",
    "**Twitter API**\n",
    " 1. There are a few data type issues but for this project we need only 3 columns: Tweet ID, retweet count and favorite count\n",
    " 2. Tweet ID 666020888022790149 is duplicated \n",
    "\n",
    "\n",
    "<span style=\"color:red; font-size:1em;\">**Tidiness:**</span>\n",
    "\n",
    "**General**\n",
    "- All tables should be part of one dataset\n",
    "\n",
    "**Twitter Archive**\n",
    "- Different stage of dogs in columns instead of rows\n",
    "\n",
    "**Twitter API**\n",
    "- For this project we need only 3 columns: Tweet ID, retweet count and favorite count. Drop the rest of columns.\n",
    "\n",
    "\n",
    "\n",
    "Assessing data quality and tidiness issue was an iterative process. While dealing with one issue, I usually found another one that was not originally listed and tried to clean it to some extent. As mentioned in the key points of this project, assessing and cleaning the entire dataset completely would require a lot of time, and is not necessary to practice and demonstrate your skills in data wrangling. I tried to be reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "### Part III - Cleaning Data\n",
    "\n",
    "I created copies of original dataframes and I tried to clean all of them separately, one after another. At the end, I inner joined the three datasets on the common key ('tweet_id') and polished a bit the joined dataframe. The dataframe was saved as a separate file \"twitter_archive_master.csv\".\n",
    "\n",
    "That was a challanging task that took me some time. There were some parts I was struggling with and needed to do an online research (which I always find as a part of learning curve). I am sure there were moments where I could have done better job with cleaning (or make the cleaning more automated, reusable with new data) or the code itself could have been cleaner, written smarter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
